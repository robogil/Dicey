{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Part 4: Technical Report\n",
    "\n",
    "#### [1. Data Collection and Adjustment](#dca)\n",
    "#### [2. Algorithm Construction](#ac)\n",
    "#### [3. Evaluation](#e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection and Adjustment <a id = dca></a>\n",
    "All data was collected from the Board Game Geek website in one form or another. The bgg.csv file contains approximately 4500 unique board games which is significantly less than the total found within the Board Game Geek API which was unfortunately discovered very recently. The user data was solely collected from the Board Game Geek API. It should also be noted that the user games were games within each user's collection on Board Game Geek with a minimum rating of 8. This will be explained further in the algorithm contruction section below.\n",
    "\n",
    "In order to pair the data properly, an inner join is required to combine the data from the .csv and the API data. The resulting dataframe makes the resulting algorithm possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bgg = pd.read_csv(\"./Cleaned_BGG\")\n",
    "bgg.drop('Unnamed: 0',axis = 1, inplace = True)\n",
    "bgg.drop_duplicates(subset= 'names', inplace= True)\n",
    "bgg['names'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # #Collect User list\n",
    "# url = \"https://boardgamegeek.com/findgamers.php?action=findclosest&country=US&srczip=07057&maxdist=100&B1=Submit\"\n",
    "# response = requests.get(url)\n",
    "# html = response.text\n",
    "\n",
    "# soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# hrefs = []\n",
    "# for link in soup.find_all('a'):\n",
    "#     hrefs.append(link.get('href'))\n",
    "    \n",
    "# # #Remove Nonetypes\n",
    "# hrefs = filter(None.__ne__, hrefs)\n",
    "# hrefs = list(hrefs)\n",
    "\n",
    "# # # Grab Usernames\n",
    "# usernames = []\n",
    "# for _ in hrefs:\n",
    "#     if re.search('\\/user\\/', _):\n",
    "#         _ = _[6:]\n",
    "#         usernames.append(_)\n",
    "# usernames = list(set(usernames))\n",
    "\n",
    "# pickle.dump(usernames, open(\"large_un.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_choices = {}\n",
    "\n",
    "# # Collect User Data\n",
    "# for user in usernames:\n",
    "#     games = []\n",
    "#     res = requests.get('http://www.boardgamegeek.com/xmlapi/collection/'+user+'?minrating=8')\n",
    "#     soup = BeautifulSoup(res.text, \"lxml\")\n",
    "#     for link in soup.find_all('name'):\n",
    "#         game = re.findall(\"(?<=name sortindex=\\\"1\\\">).*(?=</name)\", str(link))\n",
    "#         games.extend(game)\n",
    "#         user_choices.update({user:games})\n",
    "#         time.sleep(2)\n",
    "\n",
    "# #Save user choices\n",
    "# pickle.dump(user_choices, open(\"list.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create User dataframe and then combine with main game dataframe\n",
    "user_names= pickle.load(open(\"usernames.p\", \"rb\"))\n",
    "user_choices= pickle.load(open(\"gamelist.p\", \"rb\"))\n",
    "\n",
    "testlist = []\n",
    "\n",
    "for name in user_choices.keys():\n",
    "    for game in user_choices[name]:\n",
    "        testlist.append(name+'~'+game)\n",
    "        \n",
    "df_user= []\n",
    "df_game = []\n",
    "for _ in testlist:\n",
    "    user, game= _.split('~')\n",
    "    df_user.append(user)\n",
    "    df_game.append(game)\n",
    "    \n",
    "# Create User listing\n",
    "rec_user = pd.DataFrame({'User':df_user, 'Game': df_game})\n",
    "rec_user = rec_user.reindex(columns = ['User','Game'])\n",
    "rec_user.head()\n",
    "\n",
    "#Combine user data with games from main bgg dataframe\n",
    "comp = pd.merge(rec_user, bgg, how= 'inner', left_on= 'Game', right_on='names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm Contruction <a id = ac></a>\n",
    "Due to the subjective nature of the medium and recommendation engines in general, it was a goal to attempt to lessen the variability of the results. Out of all the possible features, only category and mechanic were needed to make a strong recommendation. The main problem was the incorperation of the user data into the algorithm due to it being on a completely separate scale from the mechanic and category scores which were obtained using the cosine similarity function in sklearn placing them on a -1 to 1 scale. In order to get an appropriate user score that was to scale as well as a viable metric, the game chosen would have to be in that user's collection and, to filter out all lower rated games, above a certain threshold which became a rating of 8. Once all users have been checked, all the other games within their collections are tallied and are multiplied by 0.04545 since this is strictly a tally of how many times a game appears.\n",
    "\n",
    "Once all scores have been computed, the largest problem is how to balance them all out with weights due to some scores having more recommendational strength than others. Since the goal is limiting variability, user score is the weakest followed by category, and mechanic being the strongest. This is due solely on the fact that mechanics are different flavors of rules that can intermingle but are less interpretable than the other features. Category is a broader topic and is more of a personal flavor than a defining factor for whether a person likes a game. User score is good at fitting the user in with similar users but people are fickle and tastes change.\n",
    "\n",
    "As a result, category and mechanic score are more or less dependent on the app user and their preferences. The ability to choose one or the either will be added in the future. For the time being, the mechanic score is weighted at a two times multiplier, and the category is at a one point five multiplier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up df index\n",
    "alg_rec = pd.DataFrame([])\n",
    "alg_rec['name'] = bgg['names']\n",
    "alg_rec = alg_rec.set_index('name')\n",
    "\n",
    "# User Score\n",
    "users = comp[comp['Game'] == ui ]\n",
    "users = list(users['User'])\n",
    "high_ranked= []\n",
    "\n",
    "for _ in users:\n",
    "    temp = comp[comp['User'] == _]\n",
    "    high_ranked.extend(temp.iloc[:, 1])\n",
    "high_ranked = pd.Series(high_ranked)\n",
    "high_ranked = high_ranked.value_counts()\n",
    "high_ranked = high_ranked * 0.04545453\n",
    "alg_rec['user_score'] = high_ranked\n",
    "alg_rec = alg_rec.fillna(0)\n",
    "\n",
    "# Category Score\n",
    "cat = pd.DataFrame(bgg.iloc[:,3])\n",
    "cat = cat.join(bgg.iloc[:,69:])\n",
    "cat.iloc[215:225,:]\n",
    "target= cat[cat['names'] == ui]\n",
    "cat_scores = []\n",
    "\n",
    "for _ in cat['names']:\n",
    "    test = cat[cat['names'] == _]\n",
    "    temp = cosine_similarity(target.drop('names', axis = 1), test.drop('names', axis = 1))[0]\n",
    "    cat_scores.extend(temp)\n",
    "\n",
    "alg_rec['cat_scores'] = cat_scores\n",
    "\n",
    "# Mechanic Score\n",
    "mech = pd.DataFrame(bgg.iloc[:,3])\n",
    "mech = mech.join(bgg.iloc[:,20:69])\n",
    "\n",
    "target= mech[mech['names'] == ui]\n",
    "mech_scores = []\n",
    "\n",
    "for _ in mech['names']:\n",
    "    test = mech[mech['names'] == _]\n",
    "    temp = cosine_similarity(target.drop('names', axis = 1), test.drop('names', axis = 1))[0]\n",
    "    mech_scores.extend(temp)\n",
    "    \n",
    "alg_rec['mech_scores'] = mech_scores\n",
    "\n",
    "# Final Computation\n",
    "alg_rec['mech_scores'] = alg_rec['mech_scores'] * 2.0\n",
    "alg_rec['cat_scores'] = alg_rec['cat_scores'] * 1.5\n",
    "alg_rec['total'] = alg_rec ['user_score'] + alg_rec['cat_scores'] + alg_rec['mech_scores']\n",
    "rec = alg_rec.sort_values('total', ascending= False).head(20)\n",
    "rec = list(rec.index)\n",
    "rec[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation <a id = e></a>\n",
    "The method used for evaluating the engine was to create test inputs and run them through the engine. If, at the end, the input was at the top of the list of recommendations, then the test was deemed a success. In the algorithm, user score was weighed down significantly so it wouldn't over-shadow the other two defining scores. Since all the users have the game that is being recieved as input, it will always have a larger user score along with larger category and mechanic scores since the similarity should be one to one. Out of multiple tests, this model has performed correctly one hundred percent of the time.\n",
    "\n",
    "This isn't to say there isn't room for improvement. The .csv dataset is severely and disappointingly limited. This model has already been integrated into a web app but the choices are limited to very common games and some extremely obscure games. The next step is to dramatically increase the number of games to at least twenty thousand to increase the depth and allow for the user to have more of a choice. The web app itself is also very buggy and, due to the limited data, very hard to get a prediction without have a list of model recognized games."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
